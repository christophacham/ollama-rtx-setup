# Codex CLI Configuration
# Location: C:\Users\Egusto\.codex\config.toml

# ============================================
# Model Providers Configuration
# ============================================

# Ollama Provider (Local Models)
[model_providers.ollama]
name = "Ollama"
base_url = "http://localhost:11434/v1"
wire_api = "responses"  # Fixed: This removes the deprecation warning!

# ============================================
# Model Profiles
# ============================================

# Primary coding model - Qwen2.5 Coder 32B
[profiles.qwen-code]
model_provider = "ollama"
model = "qwen2.5-coder:32b-5090"

# Deep thinking model - DeepSeek R1 32B
[profiles.deepseek]
model_provider = "ollama"
model = "deepseek-r1:32b-5090"

# General purpose - Qwen3 32B
[profiles.qwen3]
model_provider = "ollama"
model = "qwen3:32b-5090"

# Devstral - Mistral developer model
[profiles.devstral]
model_provider = "ollama"
model = "devstral-small-2:latest-5090"

# Default GPT-OSS model
[profiles.gpt-oss]
model_provider = "ollama"
model = "gpt-oss:20b"

# ============================================
# MCP Servers Configuration
# ============================================

# PAL MCP Server - Multi-model orchestration (Local Installation)
[[mcp_servers]]
name = "pal"
transport = "stdio"
# IMPORTANT: Update these paths to where you cloned pal-mcp-server
# Example: If cloned to C:\code\pal-mcp-server, use that path
command = "C:\\Users\\Egusto\\pal-mcp-server\\.pal_venv\\Scripts\\python.exe"
args = ["C:\\Users\\Egusto\\pal-mcp-server\\server.py"]
tool_timeout_sec = 1200  # 20 minutes - allows upstream providers to respond

[mcp_servers.pal.env]
PATH = "C:\\Windows\\system32;C:\\Windows;C:\\Users\\Egusto\\.local\\bin;C:\\Users\\Egusto\\AppData\\Local\\Programs\\Python\\Python312"

# Ollama configuration (local models - no API keys needed!)
OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "auto"

# Optional: Add Gemini API key if you want cloud models too
# GEMINI_API_KEY = "your-gemini-key-here"
# OPENAI_API_KEY = "your-openai-key-here"

# Optional: Disable tools you don't need
# DISABLED_TOOLS = "analyze,refactor,testgen,secaudit,docgen,tracer"

# Optional: Restrict to specific Ollama models
# OLLAMA_ALLOWED_MODELS = "qwen2.5-coder:32b-5090,deepseek-r1:32b-5090,qwen3:32b-5090"

# ============================================
# Default Settings
# ============================================

# Enable web search - required for PAL's apilookup instructions
[tools]
web_search = true

# You can set default provider and model here
# Uncomment these lines to set defaults:
# [defaults]
# provider = "ollama"
# model = "qwen2.5-coder:32b-5090"
