{
  "models": [
    {
      "model_name": "qwen3-coder:30b-5090",
      "aliases": ["qwen3-coder", "q3-coder", "agentic-coder"],
      "description": "Qwen3 Coder 30B - Agentic coding model with tool use and planning",
      "context_window": 262144,
      "max_output_tokens": 32768,
      "supports_extended_thinking": true,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 17
    },
    {
      "model_name": "qwen2.5-coder:32b-5090",
      "aliases": ["qwen-coder", "qwen-code", "coder", "qwen25-coder"],
      "description": "Qwen 2.5 Coder 32B - Best local coding model, 92 languages, rivals GPT-4o",
      "context_window": 131072,
      "max_output_tokens": 32768,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 18
    },
    {
      "model_name": "deepseek-r1:32b-5090",
      "aliases": ["deepseek-r1", "deepseek", "r1", "reasoning"],
      "description": "DeepSeek-R1 32B - Best local reasoning, approaches O3/Gemini 2.5 Pro",
      "context_window": 128000,
      "max_output_tokens": 32768,
      "supports_extended_thinking": true,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 18
    },
    {
      "model_name": "nemotron-3-nano:30b-5090",
      "aliases": ["nemotron", "nemotron-nano", "nvidia"],
      "description": "NVIDIA Nemotron 3 Nano 30B - Efficient inference with strong reasoning",
      "context_window": 32768,
      "max_output_tokens": 16384,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 14
    },
    {
      "model_name": "qwen3:32b-5090",
      "aliases": ["qwen3", "qwen", "local-qwen"],
      "description": "Qwen3 32B - Dual thinking/non-thinking modes, surpasses QwQ",
      "context_window": 131072,
      "max_output_tokens": 32768,
      "supports_extended_thinking": true,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 17
    },
    {
      "model_name": "NeuralNexusLab/CodeXor:20b",
      "aliases": ["codexor-20b", "codexor-large", "xor-20b"],
      "description": "CodeXor 20B - Advanced code generation and understanding model",
      "context_window": 32768,
      "max_output_tokens": 16384,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 15
    },
    {
      "model_name": "NeuralNexusLab/CodeXor:12b",
      "aliases": ["codexor", "codexor-12b", "xor"],
      "description": "CodeXor 12B - Efficient code-focused model for generation and review",
      "context_window": 32768,
      "max_output_tokens": 16384,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 14
    },
    {
      "model_name": "devstral:latest-5090",
      "aliases": ["devstral", "mistral-dev", "dev"],
      "description": "Devstral - Mistral's dedicated coding model for developers",
      "context_window": 32768,
      "max_output_tokens": 16384,
      "supports_extended_thinking": true,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 16
    },
    {
      "model_name": "devstral-small-2:latest-5090",
      "aliases": ["devstral-small", "devstral-s", "dev-small"],
      "description": "Devstral Small 2 - Lightweight Mistral coding model, fast inference",
      "context_window": 32768,
      "max_output_tokens": 16384,
      "supports_extended_thinking": false,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 14
    },
    {
      "model_name": "second_constantine/deepseek-coder-v2:16b-5090",
      "aliases": ["deepseek-coder-v2", "dscv2", "coder-v2"],
      "description": "DeepSeek Coder V2 16B - Strong coding benchmarks, 128K context",
      "context_window": 128000,
      "max_output_tokens": 16384,
      "supports_extended_thinking": true,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 16
    },
    {
      "model_name": "gemma3:27b",
      "aliases": ["gemma3", "gemma", "google"],
      "description": "Gemma 3 27B - Google's latest, outperforms Llama 405B on LMArena",
      "context_window": 128000,
      "max_output_tokens": 32768,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 17
    },
    {
      "model_name": "phi4:14b",
      "aliases": ["phi4", "phi", "microsoft"],
      "description": "Phi-4 14B - Microsoft's efficient model, rivals 70B on reasoning",
      "context_window": 32768,
      "max_output_tokens": 16384,
      "supports_extended_thinking": true,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 16
    },
    {
      "model_name": "codellama:34b",
      "aliases": ["codellama", "code-llama", "llama-code"],
      "description": "CodeLlama 34B - Meta's premier coding model, 20+ languages",
      "context_window": 16384,
      "max_output_tokens": 8192,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": false,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 15
    },
    {
      "model_name": "deepseek-coder:33b",
      "aliases": ["deepseek-coder", "ds-coder"],
      "description": "DeepSeek Coder 33B - Strong coding model, 87 languages",
      "context_window": 16384,
      "max_output_tokens": 8192,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 15
    },
    {
      "model_name": "qwen2.5-coder:14b",
      "aliases": ["qwen-coder-14b", "coder-14b", "qwen25-coder-14b"],
      "description": "Qwen 2.5 Coder 14B - Fast coding model with 128K context",
      "context_window": 131072,
      "max_output_tokens": 16384,
      "supports_extended_thinking": true,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 15
    },
    {
      "model_name": "qwen2.5:14b",
      "aliases": ["qwen25-14b", "qwen-14b"],
      "description": "Qwen 2.5 14B - General purpose with strong reasoning and multilingual",
      "context_window": 131072,
      "max_output_tokens": 16384,
      "supports_extended_thinking": true,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 14
    },
    {
      "model_name": "mikepfunk28/deepseekq3_agent:latest",
      "aliases": ["dsq3-agent", "deepseek-agent", "agent"],
      "description": "DeepSeek Q3 Agent - Optimized for tool use and planning tasks",
      "context_window": 32768,
      "max_output_tokens": 16384,
      "supports_extended_thinking": true,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 14
    },
    {
      "model_name": "mikepfunk28/deepseekq3_coder:latest",
      "aliases": ["dsq3-coder", "deepseek-q3-coder"],
      "description": "DeepSeek Q3 Coder - Code generation and understanding variant",
      "context_window": 32768,
      "max_output_tokens": 16384,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 14
    },
    {
      "model_name": "wizard-vicuna-uncensored:13b",
      "aliases": ["wizard-vicuna", "vicuna-uncensored", "wizard"],
      "description": "Wizard Vicuna 13B - Uncensored, strong instruction following",
      "context_window": 4096,
      "max_output_tokens": 4096,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": false,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 11
    },
    {
      "model_name": "llama3.1:8b-5090",
      "aliases": ["llama3.1", "llama-8b", "fast-search"],
      "description": "Llama 3.1 8B - Fast model for web search, tool calling enabled",
      "context_window": 131072,
      "max_output_tokens": 32768,
      "supports_extended_thinking": false,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 12
    },
    {
      "model_name": "dolphin3:8b-5090",
      "aliases": ["dolphin3", "dolphin", "uncensored"],
      "description": "Dolphin 3 8B - Uncensored helpful assistant, no safety filters",
      "context_window": 16384,
      "max_output_tokens": 8192,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 12
    },
    {
      "model_name": "dolphin-mistral:7b",
      "aliases": ["dolphin-mistral", "dolphin-7b"],
      "description": "Dolphin Mistral 7B - Uncensored Mistral fine-tune for open chat",
      "context_window": 16384,
      "max_output_tokens": 8192,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": false,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 11
    },
    {
      "model_name": "mistral:7b",
      "aliases": ["mistral", "mistral-7b", "quick"],
      "description": "Mistral 7B - Lightweight model for quick queries",
      "context_window": 32768,
      "max_output_tokens": 8192,
      "supports_extended_thinking": false,
      "supports_function_calling": true,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 10
    },
    {
      "model_name": "llama2-uncensored:7b",
      "aliases": ["llama2-uncensored", "llama2-unc", "uncensored-llama"],
      "description": "Llama 2 7B Uncensored - Open-ended discussions without filters",
      "context_window": 4096,
      "max_output_tokens": 4096,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": false,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 10
    },
    {
      "model_name": "qwen2.5:3b",
      "aliases": ["qwen-3b", "qwen25-3b", "tiny-qwen"],
      "description": "Qwen 2.5 3B - Lightweight with good general capabilities",
      "context_window": 131072,
      "max_output_tokens": 8192,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": true,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 10
    },
    {
      "model_name": "dolphin-phi:latest",
      "aliases": ["dolphin-phi", "phi-uncensored"],
      "description": "Dolphin Phi - Tiny uncensored Phi fine-tune, very fast",
      "context_window": 4096,
      "max_output_tokens": 2048,
      "supports_extended_thinking": false,
      "supports_function_calling": false,
      "supports_json_mode": false,
      "supports_images": false,
      "max_image_size_mb": 0.0,
      "intelligence_score": 9
    }
  ],
  "_README": {
    "description": "Model metadata for local/self-hosted OpenAI-compatible endpoints (Custom provider).",
    "documentation": "https://github.com/BeehiveInnovations/pal-mcp-server/blob/main/docs/custom_models.md",
    "usage": "Each entry will be advertised by the Custom provider. Aliases are case-insensitive.",
    "field_notes": "Matches providers/shared/model_capabilities.py.",
    "updated_by": "Claude Code - Synced with Ollama RTX 5090 inventory",
    "last_updated": "December 2025"
  }
}
