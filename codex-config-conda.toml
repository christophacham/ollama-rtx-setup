# Codex CLI Configuration for Windows + Conda
# Copy to: C:\Users\YOUR_USERNAME\.codex\config.toml
#
# IMPORTANT: Replace YOUR_USERNAME with your actual Windows username!
#
# Quick usage:
#   codex                    → Default (qwen2.5-coder:32b-5090)
#   codex -p fast            → Quick queries (qwen2.5:3b)
#   codex -p deepseek        → Deep reasoning (deepseek-r1:32b-5090)

# ============================================
# Model Providers Configuration
# ============================================

# Ollama Provider (Local Models)
[model_providers.ollama]
name = "Ollama"
base_url = "http://localhost:11434/v1"
wire_api = "responses"  # Fixes deprecation warning (NOT "chat")

# ============================================
# Model Profiles
# ============================================
# Quick reference:
#   -p qwen-code    → qwen2.5-coder:32b-5090 (19GB, best local coding)
#   -p deepseek     → deepseek-r1:32b-5090   (19GB, reasoning with CoT)
#   -p qwen3        → qwen3:32b-5090         (20GB, general purpose)
#   -p devstral     → devstral-small-2       (15GB, Mistral coding)
#   -p fast         → qwen2.5:3b             (2GB, quick queries)
#   -p medium       → qwen2.5-coder:14b      (9GB, balanced)
#   -p uncensored   → dolphin3:8b-5090       (5GB, uncensored chat)
#   -p codexor-20b  → CodeXor:20b            (13GB, zero-omission)
#   -p codexor-12b  → CodeXor:12b            (9GB, vision support)

# Primary coding model - Qwen2.5 Coder 32B
[profiles.qwen-code]
model_provider = "ollama"
model = "qwen2.5-coder:32b-5090"

# Deep thinking model - DeepSeek R1 32B
[profiles.deepseek]
model_provider = "ollama"
model = "deepseek-r1:32b-5090"

# General purpose - Qwen3 32B
[profiles.qwen3]
model_provider = "ollama"
model = "qwen3:32b-5090"

# Devstral - Mistral developer model
[profiles.devstral]
model_provider = "ollama"
model = "devstral-small-2:latest-5090"

# Default GPT-OSS model
[profiles.gpt-oss]
model_provider = "ollama"
model = "gpt-oss:20b"

# Fast 3B model for quick queries
[profiles.fast]
model_provider = "ollama"
model = "qwen2.5:3b"

# Medium 14B coder
[profiles.medium]
model_provider = "ollama"
model = "qwen2.5-coder:14b"

# Uncensored model
[profiles.uncensored]
model_provider = "ollama"
model = "dolphin3:8b-5090"

# CodeXor models
[profiles.codexor-20b]
model_provider = "ollama"
model = "NeuralNexusLab/CodeXor:20b"

[profiles.codexor-12b]
model_provider = "ollama"
model = "NeuralNexusLab/CodeXor:12b"

# Community finetunes
[profiles.deepseekq3-coder]
model_provider = "ollama"
model = "mikepfunk28/deepseekq3_coder:latest"

[profiles.deepseekq3-agent]
model_provider = "ollama"
model = "mikepfunk28/deepseekq3_agent:latest"

# ============================================
# MCP Servers Configuration - Using Conda
# ============================================

# PAL MCP Server - Multi-model orchestration via Conda
[mcp_servers.pal]
command = "C:\\Users\\YOUR_USERNAME\\anaconda3\\condabin\\conda.bat"
args = [
    "run",
    "-n",
    "pal-mcp",
    "--no-capture-output",
    "python",
    "C:\\Users\\YOUR_USERNAME\\code\\pal-mcp-server\\server.py"
]

# Timeouts - from official PAL guide
startup_timeout_sec = 300   # 5 minutes for conda env activation
tool_timeout_sec = 1200      # 20 minutes for PAL tool execution

[mcp_servers.pal.env]
# Ollama configuration (100% local - no API keys needed!)
OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "auto"  # Let PAL auto-select best model for each task

# Optional: Add cloud API keys if you want to mix local + cloud models
# GEMINI_API_KEY = "your-gemini-key-here"
# OPENAI_API_KEY = "your-openai-key-here"
# OPENROUTER_API_KEY = "your-openrouter-key-here"

# Optional: Disable specific tools you don't need
# DISABLED_TOOLS = "analyze,refactor,testgen,secaudit,docgen,tracer"

# Optional: Restrict PAL to specific Ollama models only
# OLLAMA_ALLOWED_MODELS = "qwen2.5-coder:32b-5090,deepseek-r1:32b-5090,qwen3:32b-5090"

# ============================================
# Features Configuration
# ============================================

# Enable web search - required for PAL's apilookup instructions to work
[features]
web_search_request = true

# ============================================
# Default Settings
# ============================================

# Default to best local coding model
[defaults]
provider = "ollama"
model = "qwen2.5-coder:32b-5090"
