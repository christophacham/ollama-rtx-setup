# Codex CLI Configuration for Virtual Environment (Non-Conda)
# Copy to: C:\Users\YOUR_USERNAME\.codex\config.toml
#
# IMPORTANT: Replace YOUR_USERNAME with your actual Windows username!
#
# Quick usage:
#   codex                    → Default (qwen2.5-coder:32b-5090)
#   codex -p fast            → Quick queries (qwen2.5:3b)
#   codex -p deepseek        → Deep reasoning (deepseek-r1:32b-5090)

# ============================================
# Model Providers Configuration
# ============================================

# Ollama Provider (Local Models)
[model_providers.ollama]
name = "Ollama"
base_url = "http://localhost:11434/v1"
wire_api = "responses"  # Fixes deprecation warning (NOT "chat")

# ============================================
# Model Profiles
# ============================================
# Quick reference:
#   -p qwen-code    → qwen2.5-coder:32b-5090 (19GB, best local coding)
#   -p deepseek     → deepseek-r1:32b-5090   (19GB, reasoning with CoT)
#   -p qwen3        → qwen3:32b-5090         (20GB, general purpose)
#   -p devstral     → devstral-small-2       (15GB, Mistral coding)
#   -p fast         → qwen2.5:3b             (2GB, quick queries)
#   -p medium       → qwen2.5-coder:14b      (9GB, balanced)

[profiles.qwen-code]
model_provider = "ollama"
model = "qwen2.5-coder:32b-5090"

[profiles.deepseek]
model_provider = "ollama"
model = "deepseek-r1:32b-5090"

[profiles.qwen3]
model_provider = "ollama"
model = "qwen3:32b-5090"

[profiles.devstral]
model_provider = "ollama"
model = "devstral-small-2:latest-5090"

[profiles.gpt-oss]
model_provider = "ollama"
model = "gpt-oss:20b"

[profiles.fast]
model_provider = "ollama"
model = "qwen2.5:3b"

[profiles.medium]
model_provider = "ollama"
model = "qwen2.5-coder:14b"

[profiles.uncensored]
model_provider = "ollama"
model = "dolphin3:8b-5090"

[profiles.codexor-20b]
model_provider = "ollama"
model = "NeuralNexusLab/CodeXor:20b"

[profiles.codexor-12b]
model_provider = "ollama"
model = "NeuralNexusLab/CodeXor:12b"

[profiles.deepseekq3-coder]
model_provider = "ollama"
model = "mikepfunk28/deepseekq3_coder:latest"

[profiles.deepseekq3-agent]
model_provider = "ollama"
model = "mikepfunk28/deepseekq3_agent:latest"

# ============================================
# MCP Servers Configuration - Using venv
# ============================================

# PAL MCP Server - Multi-model orchestration via Virtual Environment
[[mcp_servers]]
name = "pal"
transport = "stdio"
command = "C:\\Users\\YOUR_USERNAME\\code\\pal-mcp-server\\.pal_venv\\Scripts\\python.exe"
args = ["C:\\Users\\YOUR_USERNAME\\code\\pal-mcp-server\\server.py"]
tool_timeout_sec = 1200  # 20 minutes for PAL tool execution

[mcp_servers.pal.env]
# Ollama configuration (100% local - no API keys needed!)
OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "auto"  # Let PAL auto-select best model for each task

# Optional: Add cloud API keys if you want to mix local + cloud models
# GEMINI_API_KEY = "your-gemini-key-here"
# OPENAI_API_KEY = "your-openai-key-here"
# OPENROUTER_API_KEY = "your-openrouter-key-here"

# Optional: Disable specific tools you don't need
# DISABLED_TOOLS = "analyze,refactor,testgen,secaudit,docgen,tracer"

# Optional: Restrict PAL to specific Ollama models only
# OLLAMA_ALLOWED_MODELS = "qwen2.5-coder:32b-5090,deepseek-r1:32b-5090,qwen3:32b-5090"

# ============================================
# Features Configuration
# ============================================

# Enable web search - required for PAL's apilookup instructions to work
[features]
web_search_request = true

# ============================================
# Default Settings
# ============================================

# Default to best local coding model
[defaults]
provider = "ollama"
model = "qwen2.5-coder:32b-5090"
